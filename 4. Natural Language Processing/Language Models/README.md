# ðŸ§  Language Models

> Learning to model and predict language â€” from statistical n-grams to modern transformer-based approaches.

---

## ðŸ§© Concepts
Language models estimate the probability of a sequence of words.  
They are the foundation for most NLP systems today.

Topics covered:

- N-gram models and smoothing
- Recurrent networks (RNN, LSTM)
- Transformers (GPT, BERT, decoder/encoder architectures)
- Perplexity and evaluation