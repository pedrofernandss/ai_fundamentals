# üî• Deep Learning

> Understanding and implementing neural networks ‚Äî from perceptrons to convolutional and recurrent architectures.

---

## üß© Concepts

Deep Learning uses neural networks to learn hierarchical data representations.  
It builds on linear algebra and gradient-based optimization to approximate complex functions.

Core topics:
- Perceptron and Multilayer Perceptron (MLP)
- Backpropagation and gradient descent
- Optimization algorithms (SGD, Adam)
- Regularization (Dropout, BatchNorm)
- Convolutional Neural Networks (CNNs)

---

## ‚öôÔ∏è Implementation Plan

- [ ] Implement a perceptron from scratch  
- [ ] Build a feedforward MLP  
- [ ] Implement backpropagation manually  
- [ ] Visualize gradient flow  
- [ ] Compare optimizers (SGD, Adam, RMSProp)  
- [ ] Add dropout and batch normalization  
- [ ] Train CNN on MNIST or CIFAR-10  

---

## üìä Evaluation Metrics

- Accuracy  
- Loss (Training vs Validation)  
- Precision, Recall, F1  
- Confusion Matrix  
- Learning Curves  

---

## üß™ Experiments & Insights

- Compare model depth vs performance  
- Analyze overfitting with dropout  
- Visualize training loss convergence  
- Compare optimizer behavior  
- Inspect learned filters in CNNs  

---

## üìö Resources

- *Deep Learning* ‚Äî Ian Goodfellow, Yoshua Bengio & Aaron Courville  
- CS231n ‚Äî *Convolutional Neural Networks for Visual Recognition*  
- PyTorch Tutorials  
- FastAI Course ‚Äî *Practical Deep Learning for Coders*

---

## ‚úÖ Progress

- [ ] Perceptron implemented  
- [ ] MLP trained  
- [ ] Backpropagation verified  
- [ ] Optimizers tested  
- [ ] CNN trained and visualized  
